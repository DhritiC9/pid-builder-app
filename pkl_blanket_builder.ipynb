{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74d69c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optimized Processing ---\n",
      "Found 10000 files. Analyzing in parallel...\n",
      "Processing: |██████████████████████████████| 100.0% (10000/10000)\n",
      "Aggregation complete.\n",
      "Saving models to pid_models.pkl...\n",
      "--- Done in 12.05 seconds ---\n",
      "Run 'streamlit run pid_app.py' now.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict, Counter\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#Configuration\n",
    "DATASET_FOLDER = \"/Users/dhritichandan/Desktop/SFILES_2/Dhriti_Test/new_dataset/graphml_10k_dataset\"\n",
    "OUTPUT_FILE = \"pid_models.pkl\"\n",
    "MAX_NEIGHBORS_TO_SAMPLE = 75\n",
    "NUM_TOP_PATHS_TO_FIND = 5\n",
    "\n",
    "def get_node_type(node_id):\n",
    "    \"\"\"Generalizes a node ID by removing numbers.\"\"\"\n",
    "    clean_id = re.sub(r'[-\\d]+', '', node_id)\n",
    "    return clean_id if clean_id else \"unknown\"\n",
    "\n",
    "def create_blanket_entry():\n",
    "    \"\"\"Helper function to create the default dictionary structure for pickling.\"\"\"\n",
    "    return {\"parents\": Counter(), \"children\": Counter(), \"spouses\": Counter()}\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"\n",
    "    Worker function that parses AND analyzes a single GraphML file.\n",
    "    Returns the statistical models for just this one file.\n",
    "    \"\"\"\n",
    "    local_transitions = defaultdict(Counter)\n",
    "    # Fixed: Use named function instead of lambda for pickling support\n",
    "    local_blanket = defaultdict(create_blanket_entry)\n",
    "    local_paths = []\n",
    "    \n",
    "    # --- 1. Parse Graph ---\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        ns = {\"graphml\": \"http://graphml.graphdrawing.org/xmlns\"}\n",
    "        \n",
    "        # Build adjacency lists for this file locally\n",
    "        outgoing = defaultdict(list)\n",
    "        incoming = defaultdict(list)\n",
    "        nodes = {}\n",
    "\n",
    "        for node in root.findall(\".//graphml:node\", ns):\n",
    "            n_id = node.attrib[\"id\"]\n",
    "            n_type = get_node_type(n_id)\n",
    "            nodes[n_id] = n_type\n",
    "            \n",
    "        for edge in root.findall(\".//graphml:edge\", ns):\n",
    "            source = edge.attrib[\"source\"]\n",
    "            target = edge.attrib[\"target\"]\n",
    "            # Only add edges if both nodes exist in this file\n",
    "            if source in nodes and target in nodes:\n",
    "                outgoing[source].append(target)\n",
    "                incoming[target].append(source)\n",
    "\n",
    "        # --- 2. Analyze: Transitions (Markov Chain) ---\n",
    "        for node_a, neighbors_of_a in outgoing.items():\n",
    "            if not neighbors_of_a: continue\n",
    "            \n",
    "            # Sample if too many neighbors (rare in single files, but good safety)\n",
    "            sampled_a = neighbors_of_a\n",
    "            if len(neighbors_of_a) > MAX_NEIGHBORS_TO_SAMPLE:\n",
    "                sampled_a = random.sample(neighbors_of_a, MAX_NEIGHBORS_TO_SAMPLE)\n",
    "            \n",
    "            for node_b in sampled_a:\n",
    "                neighbors_of_b = outgoing.get(node_b, [])\n",
    "                if not neighbors_of_b: continue\n",
    "                \n",
    "                sampled_b = neighbors_of_b\n",
    "                if len(neighbors_of_b) > MAX_NEIGHBORS_TO_SAMPLE:\n",
    "                    sampled_b = random.sample(neighbors_of_b, MAX_NEIGHBORS_TO_SAMPLE)\n",
    "                \n",
    "                context = (nodes[node_a], nodes[node_b])\n",
    "                for node_c in sampled_b:\n",
    "                    next_node = nodes[node_c]\n",
    "                    local_transitions[context][next_node] += 1\n",
    "\n",
    "        # --- 3. Analyze: Markov Blanket (Validation) ---\n",
    "        for node_id, node_type in nodes.items():\n",
    "            # A. Parents\n",
    "            parents = incoming.get(node_id, [])\n",
    "            for p in parents:\n",
    "                local_blanket[node_type][\"parents\"][nodes[p]] += 1\n",
    "            \n",
    "            # B. Children & Spouses\n",
    "            children = outgoing.get(node_id, [])\n",
    "            for c in children:\n",
    "                c_type = nodes[c]\n",
    "                local_blanket[node_type][\"children\"][c_type] += 1\n",
    "                \n",
    "                # Spouses (parents of my child)\n",
    "                child_parents = incoming.get(c, [])\n",
    "                for spouse in child_parents:\n",
    "                    if spouse != node_id:\n",
    "                        local_blanket[node_type][\"spouses\"][nodes[spouse]] += 1\n",
    "\n",
    "        # --- 4. Find Path Candidates (for 'Start' suggestions) ---\n",
    "        # Look for Pump sequences (common starting points)\n",
    "        for node_id, node_type in nodes.items():\n",
    "            if 'pp' in node_type.lower() or 'pump' in node_type.lower():\n",
    "                # Trace back 2 steps to find a path leading to this pump\n",
    "                parents = incoming.get(node_id, [])\n",
    "                for p in parents:\n",
    "                    grandparents = incoming.get(p, [])\n",
    "                    for gp in grandparents:\n",
    "                        # Found a sequence: GP -> P -> Pump\n",
    "                        path = (nodes[gp], nodes[p], node_type)\n",
    "                        local_paths.append(path)\n",
    "\n",
    "        return (local_transitions, local_blanket, local_paths)\n",
    "\n",
    "    except Exception:\n",
    "        # Return empty structures on failure\n",
    "        return (defaultdict(Counter), defaultdict(create_blanket_entry), [])\n",
    "\n",
    "def build_and_save_models():\n",
    "    print(f\"--- Starting Optimized Processing ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.exists(DATASET_FOLDER):\n",
    "        print(f\"Error: Folder {DATASET_FOLDER} not found.\")\n",
    "        return\n",
    "\n",
    "    pid_files = [os.path.join(DATASET_FOLDER, f) for f in os.listdir(DATASET_FOLDER) \n",
    "                 if f.startswith(\"pid\") and f.endswith(\".graphml\")]\n",
    "    \n",
    "    total_files = len(pid_files)\n",
    "    print(f\"Found {total_files} files. Analyzing in parallel...\")\n",
    "\n",
    "    # Master Aggregators\n",
    "    final_transitions = defaultdict(Counter)\n",
    "    # Fixed: Use named function here too\n",
    "    final_blanket = defaultdict(create_blanket_entry)\n",
    "    final_paths = Counter()\n",
    "\n",
    "    # Parallel Execution\n",
    "    try:\n",
    "        ctx = multiprocessing.get_context('fork')\n",
    "        with ctx.Pool(processes=cpu_count()) as pool:\n",
    "            # We iterate through results AS they finish\n",
    "            for i, result in enumerate(pool.imap_unordered(process_single_file, pid_files), 1):\n",
    "                (file_trans, file_blanket, file_paths) = result\n",
    "                \n",
    "                # Aggregate Transitions\n",
    "                for ctx_key, counts in file_trans.items():\n",
    "                    final_transitions[ctx_key].update(counts)\n",
    "                \n",
    "                # Aggregate Blanket Stats\n",
    "                for n_type, stats in file_blanket.items():\n",
    "                    final_blanket[n_type][\"parents\"].update(stats[\"parents\"])\n",
    "                    final_blanket[n_type][\"children\"].update(stats[\"children\"])\n",
    "                    final_blanket[n_type][\"spouses\"].update(stats[\"spouses\"])\n",
    "                \n",
    "                # Aggregate Paths\n",
    "                final_paths.update(file_paths)\n",
    "\n",
    "                # Progress Bar\n",
    "                percent = (i / total_files) * 100\n",
    "                bar_length = 30\n",
    "                filled_length = int(bar_length * i // total_files)\n",
    "                bar = '█' * filled_length + '-' * (bar_length - filled_length)\n",
    "                sys.stdout.write(f'\\rProcessing: |{bar}| {percent:.1f}% ({i}/{total_files})')\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        print(\"\\nAggregation complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error during processing: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract Top Paths\n",
    "    top_paths = [list(p) for p, _ in final_paths.most_common(NUM_TOP_PATHS_TO_FIND)]\n",
    "\n",
    "    # Save to Disk\n",
    "    print(f\"Saving models to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"transitions\": final_transitions,\n",
    "            \"blanket_model\": dict(final_blanket),\n",
    "            \"top_paths\": top_paths\n",
    "        }, f)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"--- Done in {duration:.2f} seconds ---\")\n",
    "    print(f\"Run 'streamlit run pid_app.py' now.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_and_save_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
